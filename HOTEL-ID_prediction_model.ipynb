{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Split del training set fornito da Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset_path):\n",
    "    if 'test_images' not in os.listdir(dataset_path):\n",
    "        os.makedirs(os.path.join(dataset_path,'test_images'))\n",
    "\n",
    "    if 'val_images' not in os.listdir(dataset_path):\n",
    "        os.makedirs(os.path.join(dataset_path,'val_images'))\n",
    "\n",
    "    train_path = os.path.join(dataset_path,'train_images')\n",
    "    test_path = os.path.join(dataset_path,'test_images')\n",
    "    val_path = os.path.join(dataset_path,'val_images')\n",
    "\n",
    "    print(train_path)\n",
    "    print(test_path)\n",
    "    print(val_path)\n",
    "\n",
    "    total_files = 0\n",
    "    for folder in os.listdir(train_path):\n",
    "        for file in os.listdir(os.path.join(train_path,folder)):\n",
    "            total_files += 1\n",
    "\n",
    "    train_files = int(total_files * 0.80)                   # training sample has to be 80% of total dataset\n",
    "    test_val_files = total_files - train_files                  # validation/testing sample has to be 20%\n",
    "    \n",
    "    print(total_files)\n",
    "    print('train files are supposed to be', train_files)\n",
    "    print('test files are supposed to be', test_val_files)\n",
    "\n",
    "    cnt = 0\n",
    "    while cnt != test_val_files:\n",
    "        for folder in os.listdir(train_path):\n",
    "            if (len(os.listdir(os.path.join(train_path, folder))) != 0):\n",
    "                file = random.choice(os.listdir(os.path.join(train_path, folder)))\n",
    "                if os.path.isfile(os.path.join(train_path,folder,file)):\n",
    "                    # copies the file in test folder\n",
    "                    shutil.copy(os.path.join(train_path,folder,file), os.path.join(test_path,str(cnt)+'.jpg'))\n",
    "\n",
    "                    # creates a folder with that hotel id (if it doesn't exist) and moves the image file into it\n",
    "                    if folder not in os.listdir(val_path):\n",
    "                        os.makedirs(os.path.join(val_path, folder))\n",
    "                    shutil.move(os.path.join(train_path,folder,file), os.path.join(val_path, folder))\n",
    "\n",
    "                    cnt += 1\n",
    "                    if cnt == test_val_files:\n",
    "                        break\n",
    "\n",
    "\n",
    "    for folder in os.listdir(train_path):\n",
    "        if len(os.listdir(os.path.join(train_path,folder))) == 0:\n",
    "            print(\"Directory\", os.path.join(train_path,folder), \"will be deleted as it's empty\")\n",
    "            os.rmdir(os.path.join(train_path,folder))\n",
    "\n",
    "    return train_path, val_path, test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv(train_path, test_path, val_path):\n",
    "    header_train = ['image_id', 'hotel_id']\n",
    "    \n",
    "    with open(os.path.join(dataset_path,'train.csv'), 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header_train)\n",
    "        for hotel_id in os.listdir(train_path):\n",
    "            for image_id in os.listdir(os.path.join(train_path, hotel_id)):\n",
    "                writer.writerow([image_id, hotel_id])\n",
    "\n",
    "    with open(os.path.join(dataset_path,'val.csv'), 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header_train)\n",
    "        for hotel_id in os.listdir(val_path):\n",
    "            for image_id in os.listdir(os.path.join(val_path, hotel_id)):\n",
    "                writer.writerow([image_id, hotel_id])\n",
    "\n",
    "    with open(os.path.join(dataset_path,'test.csv'), 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['image_id'])\n",
    "        for image_id in os.listdir(test_path):\n",
    "            writer.writerow([image_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images\n",
      "/run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/test_images\n",
      "/run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/val_images\n",
      "44703\n",
      "train files are supposed to be 35762\n",
      "test files are supposed to be 8941\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/111413 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/1122 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/16468 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/11359 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/12178 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/12381 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/12414 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/13132 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/139916 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/18718 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/18855 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/19184 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/197776 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/200002 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/200215 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/201139 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/201241 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/203817 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/206515 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/207157 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/211268 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/22091 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/23460 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/236304 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/25213 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/25543 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/256039 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/28200 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/30552 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/307643 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/308982 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/309234 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/309302 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/309506 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/310041 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/309431 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/3187 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/34250 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/35005 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/35402 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/35579 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/35677 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/396607 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/310554 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/53133 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/63933 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/688391 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/689550 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/723 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/73309 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/74982 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/75014 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/75137 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/47944 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/82686 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/84360 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/97025 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/93959 will be deleted as it's empty\n",
      "Directory /run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/95159 will be deleted as it's empty\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join('/run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/')\n",
    "\n",
    "train_path, val_path, test_path = split_dataset(dataset_path)\n",
    "generate_csv(train_path, test_path, val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: riga 1: nvidia-smi: comando non trovato\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================= ROCm System Management Interface =======================\n",
      "================================= Product Info =================================\n",
      "GPU[0]\t\t: Card series: \t\tNavi 14 [Radeon RX 5500/5500M / Pro 5500M]\n",
      "GPU[0]\t\t: Card model: \t\t0x04e6\n",
      "GPU[0]\t\t: Card vendor: \t\tAdvanced Micro Devices, Inc. [AMD/ATI]\n",
      "GPU[0]\t\t: Card SKU: \t\tD332P2\n",
      "================================================================================\n",
      "============================= End of ROCm SMI Log ==============================\n"
     ]
    }
   ],
   "source": [
    "!rocm-smi --showproductname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Caricamento del set di test e di training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michele/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "import albumentations.pytorch as APT\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join('/run/media/michele/Archivio/DL_project/hotel-id-to-combat-human-trafficking-2022-fgvc9/')\n",
    "\n",
    "train_path = os.path.join(dataset_path, 'train_images')\n",
    "test_path = os.path.join(dataset_path, 'test_images')\n",
    "val_path = os.path.join(dataset_path, 'val_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import albumentations.pytorch as APT\n",
    "\n",
    "IMG_SIZE = 128\n",
    "\n",
    "occlusion_transform = A.CoarseDropout(p=1, max_holes=1, min_height=IMG_SIZE//4, max_height=IMG_SIZE//2,\n",
    "                                      min_width=IMG_SIZE//4, max_width=IMG_SIZE//2, fill_value=(255, 0, 0))\n",
    "\n",
    "# transform used for the obtained training set\n",
    "train_transform = A.Compose([A.HorizontalFlip(p=0.75),\n",
    "                            A.VerticalFlip(p=0.25),\n",
    "                            A.ShiftScaleRotate(p=0.5),\n",
    "                            A.OpticalDistortion(p=0.25),\n",
    "                            A.Perspective(p=0.25),\n",
    "                            A.CoarseDropout(p=0.5, min_holes=1, max_holes=6, \n",
    "                                            min_height=IMG_SIZE//16, max_height=IMG_SIZE//4,\n",
    "                                            min_width=IMG_SIZE//16, max_width=IMG_SIZE//4),\n",
    "                            occlusion_transform,\n",
    "                            A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "                            A.RandomBrightnessContrast(p=0.75),\n",
    "                            A.ToFloat(),\n",
    "                            APT.transforms.ToTensorV2()])\n",
    "\n",
    "# transform used for the validation/test set\n",
    "val_transform = A.Compose([occlusion_transform,\n",
    "                            A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "                            A.ToFloat(),\n",
    "                            APT.transforms.ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HotelDataset:\n",
    "    def __init__(self, data, transform=None, data_path=train_path, train_val=True):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.data_path = data_path\n",
    "        self.train_val = train_val\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.data.iloc[idx]\n",
    "\n",
    "        if self.train_val:\n",
    "            image_path = os.path.join(self.data_path,str(record['hotel_id']),record['image_id'])\n",
    "        else:\n",
    "            image_path = os.path.join(self.data_path,record['image_id'])\n",
    "        \n",
    "        image = np.array(Image.open(image_path)).astype(np.uint8)\n",
    "        label = record['hotel_id']\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "        \n",
    "        if self.train_val:\n",
    "            return image, label\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class PairHotelDataset(data.Dataset):\n",
    "    def __init__(self, data, transform=None, data_path=train_path, train_val=True):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.data_path = data_path\n",
    "        self.train_val = train_val\n",
    "\n",
    "        self.indices = data['hotel_id'].unique()\n",
    "        self.indices = self.indices.tolist()\n",
    "\n",
    "        self.generate_pairs()\n",
    "\n",
    "    def generate_pairs(self):\n",
    "        self.pair_labels = (np.random.rand(len(self.data))>0.5).astype(int)\n",
    "\n",
    "        self.paired_idx = []\n",
    "\n",
    "        for i, l in enumerate(self.pair_labels):\n",
    "            c1 = self.data.loc[i]['hotel_id'].item()\n",
    "            c = c1\n",
    "\n",
    "            if l == 0:\n",
    "                j = np.random.choice(os.listdir(os.path.join(self.data_path,str(c1))))\n",
    "            else:\n",
    "                diff_class = np.random.choice(list(set(self.indices)-{c1}))\n",
    "                j = np.random.choice(os.listdir(os.path.join(self.data_path,str(diff_class))))\n",
    "                c = diff_class\n",
    "            self.paired_idx.append({'image_id': j, 'hotel_id': c})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record1 = self.data.iloc[idx]\n",
    "        record2 = self.paired_idx[idx]\n",
    "\n",
    "        if self.train_val:\n",
    "            im1_path = os.path.join(self.data_path,str(record1['hotel_id']),record1['image_id'])\n",
    "            im2_path = os.path.join(self.data_path,str(record2['hotel_id']),record2['image_id'])\n",
    "        else:\n",
    "            im1_path = os.path.join(self.data_path,str(record1['image_id']))\n",
    "            im2_path = os.path.join(self.data_path,str(record2['image_id']))\n",
    "        \n",
    "        im1 = np.array(Image.open(im1_path)).astype(np.uint8)\n",
    "        l1 = record1['hotel_id']\n",
    "\n",
    "        im2 = np.array(Image.open(im2_path)).astype(np.uint8)\n",
    "        l2 = record2['hotel_id']\n",
    "\n",
    "        l = self.pair_labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            transformed1 = self.transform(image=im1)\n",
    "            im1 = transformed1['image']\n",
    "\n",
    "            transformed2 = self.transform(image=im2)\n",
    "            im2 = transformed2['image']\n",
    "\n",
    "        return im1, im2, l, l1, l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Definizione del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - HotelPredictionNetwork v1 (only classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HotelPredictionNetwork(pl.LightningModule):\n",
    "    def __init__(self, n_classes, embedding_size, extractor_name='efficientnet_b0'):\n",
    "        super(HotelPredictionNetwork, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.extractor = timm.create_model(extractor_name, num_classes=n_classes, pretrained=True)\n",
    "        in_features = self.extractor.get_classifier().in_features\n",
    "\n",
    "        self.extractor.classifier = nn.Identity()\n",
    "\n",
    "        self.embedding = nn.Linear(in_features, embedding_size)\n",
    "        self.classifier = nn.Linear(embedding_size, n_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extractor(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def embed_and_classify(self, x):\n",
    "        x = self.forward(x)\n",
    "        return x, self.classifier(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = SGD(self.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        _, output = self.embed_and_classify(x)\n",
    "        loss = self.criterion(output, y)\n",
    "        self.log('train/loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        output = self.forward(x)\n",
    "\n",
    "        acc = accuracy_score(y.cpu(), output.cpu().topk(1).indices)\n",
    "\n",
    "        self.log('val/accuracy', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - HotelPredictionNetworkv2 - Metric Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, m=2):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.m = m\n",
    "\n",
    "    def forward(self, phi_i, phi_j, l_ij):\n",
    "        l_ij = l_ij.float()\n",
    "        d = F.pairwise_distance(phi_i, phi_j)\n",
    "        l = (0.5 * (1 - l_ij) * torch.pow(d, 2)) + (0.5 * l_ij * torch.pow(torch.clamp(self.m - d, min=0),2))\n",
    "\n",
    "        return l.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HotelPredictionNetworkv2(pl.LightningModule):\n",
    "    def __init__(self, n_classes, extractor_name='efficientnet_b0', lr=0.01, momentum=0.99, margin=2):\n",
    "        super(HotelPredictionNetworkv2, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.embedding = timm.create_model(extractor_name, num_classes=n_classes, pretrained=True)\n",
    "        self.embedding.classifier = nn.Identity()\n",
    "\n",
    "        self.criterion = ContrastiveLoss(margin)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = SGD(self.embedding.parameters(), self.hparams.lr, momentum=self.hparams.momentum)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        I_i, I_j, l_ij, *_ = train_batch\n",
    "\n",
    "        phi_i = self.embedding(I_i)\n",
    "        phi_j = self.embedding(I_j)\n",
    "\n",
    "        loss = self.criterion(phi_i, phi_j, l_ij)\n",
    "\n",
    "        self.log('train/loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        I_i, I_j, l_ij, *_ = val_batch\n",
    "\n",
    "        phi_i = self.embedding(I_i)\n",
    "        phi_j = self.embedding(I_j)\n",
    "\n",
    "        loss = self.criterion(phi_i, phi_j, l_ij)\n",
    "\n",
    "        self.log('val/loss', loss)\n",
    "\n",
    "        if batch_idx==0:\n",
    "            self.logger.experiment.add_embedding(phi_i, val_batch[3], I_i, global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(dataset_path, 'train.csv'))\n",
    "df_val = pd.read_csv(os.path.join(dataset_path, 'val.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_train = HotelDataset(df_train, train_transform, train_path)\n",
    "hotel_val = HotelDataset(df_val, val_transform, val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_train_loader = DataLoader(hotel_train, batch_size=16, num_workers=6, shuffle=True)\n",
    "hotel_val_loader = DataLoader(hotel_val, batch_size=16, num_workers=6)\n",
    "\n",
    "#print(next(iter(hotel_train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = next(iter(hotel_train_loader))\n",
    "images = dataiter[0]\n",
    "labels = dataiter[1]\n",
    "\n",
    "# print(images.shape)\n",
    "\n",
    "# for i in range(len(images)):\n",
    "#     for c in range(i):\n",
    "#         plt.imshow(images[i][c])\n",
    "        \n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690366\n",
      "690365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "n_classes_train = df_train['hotel_id'].max()\n",
    "n_classes_val = df_val['hotel_id'].max()\n",
    "\n",
    "n_classes = max(n_classes_train, n_classes_val) + 1\n",
    "print(n_classes)\n",
    "\n",
    "print(n_classes_val.min(0))\n",
    "\n",
    "model = HotelPredictionNetwork(n_classes, 128)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=1, accelerator=\"gpu\", devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | extractor  | EfficientNet     | 4.0 M \n",
      "1 | embedding  | Linear           | 163 K \n",
      "2 | classifier | Linear           | 89.1 M\n",
      "3 | criterion  | CrossEntropyLoss | 0     \n",
      "------------------------------------------------\n",
      "93.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "93.2 M    Total params\n",
      "372.915   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2795/2795 [41:58<00:00,  1.11it/s, loss=13.3, v_num=14] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2795/2795 [41:59<00:00,  1.11it/s, loss=13.3, v_num=14]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, hotel_train_loader, hotel_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 559/559 [06:01<00:00,  1.55it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val/accuracy                  0.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val/accuracy': 0.0}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, hotel_val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(dataset_path, 'train.csv'))\n",
    "df_val = pd.read_csv(os.path.join(dataset_path, 'val.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_hotel_train = PairHotelDataset(df_train, train_transform, train_path)\n",
    "pair_hotel_val = PairHotelDataset(df_val, val_transform, val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_hotel_train_loader = DataLoader(pair_hotel_train, batch_size=64, num_workers=8, shuffle=True)\n",
    "pair_hotel_val_loader = DataLoader(pair_hotel_val, batch_size=64, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "n_classes_train = df_train['hotel_id'].max()\n",
    "n_classes_val = df_val['hotel_id'].max()\n",
    "\n",
    "n_classes = max(n_classes_train, n_classes_val) + 1\n",
    "\n",
    "siamese_hotel_task = HotelPredictionNetworkv2(n_classes)\n",
    "logger = TensorBoardLogger(\"metric_logs\", name=\"siamese_hotel_id\")\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1, logger=logger, max_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type            | Params\n",
      "----------------------------------------------\n",
      "0 | embedding | EfficientNet    | 4.0 M \n",
      "1 | criterion | ContrastiveLoss | 0     \n",
      "----------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.030    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 699/699 [54:38<00:00,  4.69s/it, loss=0.525, v_num=9]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 699/699 [54:38<00:00,  4.69s/it, loss=0.525, v_num=9]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(siamese_hotel_task, pair_hotel_train_loader, pair_hotel_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:   0%|          | 0/140 [00:00<?, ?it/s]warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "Validation DataLoader 0: 100%|██████████| 140/140 [09:01<00:00,  3.87s/it]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        val/loss            0.8493420481681824\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val/loss': 0.8493420481681824}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(siamese_hotel_task, pair_hotel_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "2023-01-02 20:19:29.194397: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
       "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
       "2023-01-02 20:19:30.128270: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/michele/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
       "2023-01-02 20:19:30.128355: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/michele/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
       "2023-01-02 20:19:30.128364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
       "Error: A logdir or db must be specified. For example `tensorboard --logdir mylogdir` or `tensorboard --db sqlite:~/.tensorboard.db`. Run `tensorboard --helpfull` for details and examples."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir metric_logs/siamese_hotel_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
